```py
"""
ä½¿ç”¨æœ¬åœ° Qwen2.5-0.5B-Instruct æ¨¡å‹çš„ RAG Pipeline
"""

import json
import os
import torch
from typing import Dict, Any, List, Optional
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

from local_model_config import (
    LOCAL_MODEL_PATH,
    GENERATION_CONFIG,
    CRITIQUE_GEN_CONFIG,
    REWRITE_GEN_CONFIG,
    CLASSIFICATION_CONFIG,
    CATEGORIES,
    CATEGORY_MAPPING,
    CLASSIFICATION_PROMPT,
    SYSTEM_PROMPT,
    CRITIQUE_PROMPT,
    REWRITE_PROMPT,
    STRUCTURED_LAWS_PATH,
    TOP_K,
    extract_and_map_categories,
    get_random_critique_request,
    CRITIQUE
)


class LocalModelPipeline:
    """ä½¿ç”¨æœ¬åœ°æ¨¡å‹çš„å®Œæ•´RAG Pipeline"""
    
    def __init__(self, model_path: str = LOCAL_MODEL_PATH, use_embedding: bool = True, use_vllm: bool = False):
        """
        åˆå§‹åŒ–Pipeline
        
        Args:
            model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„
            use_embedding: æ˜¯å¦ä½¿ç”¨å‘é‡åŒ–æ£€ç´¢ï¼ˆé»˜è®¤Trueï¼Œæ¨èï¼‰
            use_vllm: æ˜¯å¦ä½¿ç”¨vLLMåŠ é€Ÿï¼ˆé»˜è®¤Falseï¼Œå¦‚æœTrueåˆ™ä½¿ç”¨vLLMè¿›è¡Œæ¨ç†åŠ é€Ÿï¼‰
        """
        print(f"æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {model_path}")
        
        # éªŒè¯æ¨¡å‹è·¯å¾„
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        
        config_path = os.path.join(model_path, "config.json")
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"æ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        print(f"  æ¨¡å‹è·¯å¾„éªŒè¯é€šè¿‡: {model_path}")
        
        # åŠ è½½æ³•å¾‹æ•°æ®å’Œç±»åˆ«ï¼ˆæ— è®ºä½¿ç”¨å“ªç§æ¨ç†å¼•æ“éƒ½éœ€è¦ï¼‰
        with open(STRUCTURED_LAWS_PATH, 'r', encoding='utf-8') as f:
            self.laws_by_category = json.load(f)
        
        print(f"âœ“ å·²åŠ è½½æ³•å¾‹æ•°æ®: {sum(len(laws) for laws in self.laws_by_category.values())} æ¡æ³•è§„")
        
        self.categories = CATEGORIES
        
        # åˆå§‹åŒ–å‘é‡åŒ–æ£€ç´¢ç›¸å…³å±æ€§ï¼ˆæ— è®ºä½¿ç”¨å“ªç§æ¨ç†å¼•æ“éƒ½éœ€è¦ï¼‰
        self.use_embedding = use_embedding
        self.embedding_retriever = None
        
        self.use_vllm = use_vllm
        
        if use_vllm:
            # ä½¿ç”¨ vLLM åŠ é€Ÿæ¨ç†
            try:
                from vllm import LLM
                print("  ä½¿ç”¨ vLLM åŠ é€Ÿæ¨ç†...")
                self.llm = LLM(
                    model=model_path,
                    trust_remote_code=True,
                    dtype="bfloat16"
                )
                # vLLM ä¼šè‡ªåŠ¨åŠ è½½ tokenizer
                self.tokenizer = self.llm.get_tokenizer()
                self.model = None  # vLLM æ¨¡å¼ä¸‹ä¸ä½¿ç”¨ transformers æ¨¡å‹
                print(f"âœ“ vLLM æ¨¡å‹åŠ è½½æˆåŠŸ")
                
                # åˆå§‹åŒ–å‘é‡åŒ–æ£€ç´¢å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.use_embedding:
                    self._init_embedding_retriever()
            except ImportError:
                print("  âš ï¸ vLLM æœªå®‰è£…ï¼Œå›é€€åˆ° transformers")
                print("  å®‰è£… vLLM: pip install vllm")
                self.use_vllm = False
                self._load_transformers_model(model_path)
            except Exception as e:
                print(f"  âš ï¸ vLLM åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå›é€€åˆ° transformers")
                self.use_vllm = False
                self._load_transformers_model(model_path)
        else:
            # ä½¿ç”¨ transformersï¼ˆé»˜è®¤ï¼‰
            self._load_transformers_model(model_path)
    
    def _load_transformers_model(self, model_path: str):
        """åŠ è½½ transformers æ¨¡å‹"""
        # åŠ è½½tokenizerå’Œæ¨¡å‹ï¼ˆä½¿ç”¨local_files_onlyé¿å…è”ç½‘ï¼‰
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True,
            local_files_only=True  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
            local_files_only=True  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )
        self.llm = None
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (è®¾å¤‡: {self.model.device})")
        
        # åˆå§‹åŒ–å‘é‡åŒ–æ£€ç´¢å™¨ï¼ˆå¦‚æœè¿˜æœªåˆå§‹åŒ–ï¼‰
        if self.use_embedding and self.embedding_retriever is None:
            self._init_embedding_retriever()
    
    def _init_embedding_retriever(self):
        """åˆå§‹åŒ–å‘é‡åŒ–æ£€ç´¢å™¨"""
        try:
            from embedding_retrieval import EmbeddingRetriever
            print("\næ­£åœ¨åŠ è½½å‘é‡åŒ–æ£€ç´¢æ¨¡å—...")
            # è®¾ç½®ç´¢å¼•ç¼“å­˜ç›®å½•ï¼ˆç”¨äºåŠ é€Ÿåç»­åŠ è½½ï¼‰
            current_dir = os.path.dirname(os.path.abspath(__file__))
            index_cache_dir = os.path.join(current_dir, "faiss_index_cache")
            self.embedding_retriever = EmbeddingRetriever(
                STRUCTURED_LAWS_PATH,
                index_cache_dir=index_cache_dir
            )
            print("âœ“ å‘é‡åŒ–æ£€ç´¢å·²å¯ç”¨")
        except ImportError as e:
            print(f"\nâŒ æ— æ³•å¯ç”¨å‘é‡åŒ–æ£€ç´¢: {e}")
            print("\nè¯·å®‰è£…ä¾èµ–ï¼š")
            print("  pip install sentence-transformers faiss-cpu")
            print("æˆ–ï¼š")
            print("  pip install -r requirements_embedding.txt")
            print("\nâš ï¸ å°†ä½¿ç”¨å…³é”®è¯æ£€ç´¢ï¼ˆæ•ˆæœè¾ƒå·®ï¼‰")
            self.use_embedding = False
        except Exception as e:
            print(f"\nâŒ å‘é‡åŒ–æ£€ç´¢åˆå§‹åŒ–å¤±è´¥: {e}")
            print("âš ï¸ å°†ä½¿ç”¨å…³é”®è¯æ£€ç´¢ï¼ˆæ•ˆæœè¾ƒå·®ï¼‰")
            self.use_embedding = False
    
    def generate_text_batch(self, prompts: List[str], config: Dict = None, system_prompt: str = None) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡æœ¬ï¼ˆä½¿ç”¨Qwençš„im_start/im_endæ ¼å¼ï¼‰
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            config: ç”Ÿæˆé…ç½®
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        if config is None:
            config = GENERATION_CONFIG
        
        # æ„å»ºæ‰¹é‡prompt
        texts = []
        for prompt in prompts:
            if system_prompt:
                text = f"""<|im_start|>system
{system_prompt}
<|im_end|>
<|im_start|>user
{prompt}
<|im_end|>
<|im_start|>assistant
"""
            else:
                text = f"""<|im_start|>user
{prompt}
<|im_end|>
<|im_start|>assistant
"""
            texts.append(text)
        
        if self.use_vllm:
            # ä½¿ç”¨ vLLM æ‰¹é‡æ¨ç†
            from vllm import SamplingParams
            
            sampling_params = SamplingParams(
                max_tokens=config["max_new_tokens"],
                temperature=config["temperature"],
                top_p=config["top_p"],
                top_k=config["top_k"],
                repetition_penalty=config["repetition_penalty"],
                stop=["<|im_end|>", "<|endoftext|>"]  # Qwen åœæ­¢ç¬¦
            )
            
            outputs = self.llm.generate(texts, sampling_params)
            responses = [output.outputs[0].text.strip() for output in outputs]
            return responses
        else:
            # ä½¿ç”¨ transformers æ‰¹é‡æ¨ç†
            inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True).to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=config["max_new_tokens"],
                    temperature=config["temperature"],
                    top_p=config["top_p"],
                    top_k=config["top_k"],
                    repetition_penalty=config["repetition_penalty"],
                    do_sample=config["do_sample"],
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç ï¼ˆåªè¿”å›æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
            responses = []
            for i, output in enumerate(outputs):
                input_length = inputs['input_ids'][i].shape[0]
                generated_ids = output[input_length:]
                response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
                responses.append(response.strip())
            
            return responses
    
    def generate_text(self, prompt: str, config: Dict = None, system_prompt: str = None) -> str:
        """
        ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼ˆä½¿ç”¨Qwençš„im_start/im_endæ ¼å¼ï¼‰
        
        Args:
            prompt: è¾“å…¥æç¤º
            config: ç”Ÿæˆé…ç½®
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if config is None:
            config = GENERATION_CONFIG
        
        # ç›´æ¥ä½¿ç”¨ Qwen çš„ im_start/im_end æ ¼å¼
        if system_prompt:
            text = f"""<|im_start|>system
{system_prompt}
<|im_end|>
<|im_start|>user
{prompt}
<|im_end|>
<|im_start|>assistant
"""
        else:
            text = f"""<|im_start|>user
{prompt}
<|im_end|>
<|im_start|>assistant
"""
        
        if self.use_vllm:
            # ä½¿ç”¨ vLLM åŠ é€Ÿæ¨ç†
            from vllm import SamplingParams
            
            sampling_params = SamplingParams(
                max_tokens=config["max_new_tokens"],
                temperature=config["temperature"],
                top_p=config["top_p"],
                top_k=config["top_k"],
                repetition_penalty=config["repetition_penalty"],
                stop=["<|im_end|>", "<|endoftext|>"]  # Qwen åœæ­¢ç¬¦
            )
            
            outputs = self.llm.generate([text], sampling_params)
            response = outputs[0].outputs[0].text.strip()
            return response
        else:
            # ä½¿ç”¨ transformersï¼ˆåŸå§‹æ–¹å¼ï¼‰
            # Tokenize
            inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=config["max_new_tokens"],
                    temperature=config["temperature"],
                    top_p=config["top_p"],
                    top_k=config["top_k"],
                    repetition_penalty=config["repetition_penalty"],
                    do_sample=config["do_sample"],
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç ï¼ˆåªè¿”å›æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
            generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
            response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
            
            return response.strip()
    
    def classify_question(self, question: str, use_llm: bool = True) -> tuple:
        """
        é—®é¢˜åˆ†ç±»ï¼ˆæ”¯æŒå¤šç±»åˆ«ï¼‰
        
        ä½¿ç”¨ CLASSIFICATION_PROMPT è°ƒç”¨æ¨¡å‹è¿›è¡Œåˆ†ç±»
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_llm: æ˜¯å¦ä½¿ç”¨LLMåˆ†ç±»ï¼ˆé»˜è®¤Trueï¼Œä½¿ç”¨CLASSIFICATION_PROMPTï¼‰
            
        Returns:
            (categories, confidence, reason) - categoriesæ˜¯ç±»åˆ«åˆ—è¡¨
        """
        # å§‹ç»ˆä½¿ç”¨ LLM åˆ†ç±»ï¼ˆé€šè¿‡ CLASSIFICATION_PROMPTï¼‰
        categories_str = "\n".join([
            f"{i+1}. {name}ï¼š{info['description']}"
            for i, (name, info) in enumerate(self.categories.items())
        ])
        
        prompt = CLASSIFICATION_PROMPT.format(
            question=question,
            categories_str=categories_str
        )
        
        try:
            # ä½¿ç”¨ CLASSIFICATION_PROMPT è°ƒç”¨æ¨¡å‹
            response = self.generate_text(prompt, CLASSIFICATION_CONFIG)
            
            # ä½¿ç”¨extract_and_map_categoriesæå–å¤šä¸ªç±»åˆ«
            categories = extract_and_map_categories(response)
            
            if not categories:
                print(f"  âš ï¸ æ¨¡å‹æœªè¿”å›æœ‰æ•ˆç±»åˆ«ï¼ŒåŸå§‹è¾“å‡º: {response}")
                # å¦‚æœæå–å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç±»åˆ«
                return ["å†›é˜Ÿç»„ç»‡ä¸ç®¡ç†"], 0.3, "æ¨¡å‹åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«"
            
            # éªŒè¯ç±»åˆ«æ˜¯å¦éƒ½åœ¨CATEGORY_MAPPINGä¸­
            valid_categories = [cat for cat in categories if cat in CATEGORY_MAPPING]
            if not valid_categories:
                print(f"  âš ï¸ æ¨¡å‹è¿”å›çš„ç±»åˆ«æ— æ•ˆ: {categories}ï¼ŒåŸå§‹è¾“å‡º: {response}")
                # å¦‚æœç±»åˆ«æ— æ•ˆï¼Œè¿”å›é»˜è®¤ç±»åˆ«
                return ["å†›é˜Ÿç»„ç»‡ä¸ç®¡ç†"], 0.3, "æ¨¡å‹è¿”å›æ— æ•ˆç±»åˆ«ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«"
            
            confidence = 0.7 if len(valid_categories) == 1 else 0.6
            reason = f"æ¨¡å‹åˆ†ç±»({len(valid_categories)}ä¸ªç±»åˆ«)"
            
            return valid_categories, confidence, reason
            
        except Exception as e:
            print(f"  âš ï¸ æ¨¡å‹åˆ†ç±»å¤±è´¥: {e}")
            # åˆ†ç±»å¤±è´¥æ—¶è¿”å›é»˜è®¤ç±»åˆ«
            return ["å†›é˜Ÿç»„ç»‡ä¸ç®¡ç†"], 0.3, f"åˆ†ç±»å¼‚å¸¸: {str(e)}"
    
    def _keyword_classify(self, question: str) -> tuple:
        """å…³é”®è¯åˆ†ç±»"""
        question_lower = question.lower()
        scores = {}
        
        for cat, info in self.categories.items():
            score = 0
            keywords = info["keywords"]
            
            # å‰3ä¸ªå…³é”®è¯æƒé‡3
            for kw in keywords[:3]:
                if kw.lower() in question_lower:
                    score += 3
            
            # å…¶ä½™å…³é”®è¯æƒé‡1
            for kw in keywords[3:]:
                if kw.lower() in question_lower:
                    score += 1
            
            scores[cat] = score
        
        best_cat = max(scores, key=scores.get)
        best_score = scores[best_cat]
        
        if best_score > 0:
            confidence = min(0.7, best_score * 0.15)
        else:
            best_cat = "å†›é˜Ÿç»„ç»‡ä¸ç®¡ç†"
            confidence = 0.3
        
        reason = f"å…³é”®è¯åŒ¹é…(å¾—åˆ†{best_score})"
        
        return best_cat, confidence, reason
    
    def retrieve_laws(self, question: str, category: str, top_k: int = TOP_K) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³æ³•è§„ï¼ˆæ”¹è¿›çš„å…³é”®è¯åŒ¹é…ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            category: ç±»åˆ«
            top_k: è¿”å›æ•°é‡
            
        Returns:
            æ³•è§„åˆ—è¡¨
        """
        if category not in self.laws_by_category:
            return []
        
        laws = self.laws_by_category[category]
        question_lower = question.lower()
        
        # æå–é—®é¢˜ä¸­çš„å…³é”®è¯ï¼ˆå»é™¤åœç”¨è¯ï¼‰
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
        question_words = [w for w in question_lower.split() if w not in stop_words and len(w) > 1]
        
        scores = []
        for law in laws:
            score = 0
            
            # 1. prohibited_actionsç²¾ç¡®åŒ¹é… â†’ æƒé‡Ã—10ï¼ˆæé«˜æƒé‡ï¼‰
            for action in law.get("prohibited_actions", []):
                action_lower = action.lower()
                # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´è¯ç»„åŒ¹é…
                for qword in question_words:
                    if qword in action_lower:
                        score += 10
                # æ£€æŸ¥å¤šå­—åŒ¹é…
                if any(word in action_lower for word in question_words if len(word) >= 2):
                    score += 5
            
            # 2. summaryå…³é”®è¯åŒ¹é… â†’ æƒé‡Ã—3
            summary = law.get("summary", "").lower()
            for qword in question_words:
                if qword in summary:
                    score += 3
            
            # 3. titleåŒ¹é… â†’ æƒé‡Ã—5
            title = law.get("title", "").lower()
            for qword in question_words:
                if qword in title:
                    score += 5
            
            # 4. full_textæ·±åº¦åŒ¹é… â†’ æƒé‡Ã—2
            full_text = law.get("full_text", "").lower()
            for qword in question_words:
                if qword in full_text:
                    score += 2
            
            # 5. ç±»åˆ«å…³é”®è¯åŒ¹é…ï¼ˆä»CATEGORIESè·å–ï¼‰
            if category in self.categories:
                cat_keywords = self.categories[category].get("keywords", [])
                for keyword in cat_keywords:
                    if keyword.lower() in question_lower:
                        score += 1
            
            scores.append((law, score))
        
        # æ’åºå¹¶è¿”å›Top-K
        scores.sort(key=lambda x: x[1], reverse=True)
        
        # å¦‚æœæ‰€æœ‰å¾—åˆ†éƒ½æ˜¯0ï¼Œè¿”å›è¯¥ç±»åˆ«çš„å‰top_kæ¡æ³•è§„ï¼ˆå…œåº•ç­–ç•¥ï¼‰
        if all(score == 0 for _, score in scores):
            print(f"  âš ï¸ å…³é”®è¯åŒ¹é…æ— ç»“æœï¼Œè¿”å›{category}çš„å‰{top_k}æ¡æ³•è§„")
            return laws[:top_k]
        
        return [law for law, score in scores[:top_k] if score > 0]
    
    def format_laws(self, laws: List[Dict]) -> str:
        """æ ¼å¼åŒ–æ³•è§„ä¸ºPromptæ–‡æœ¬ï¼ˆç®€æ´ç‰ˆï¼‰"""
        if not laws:
            return "ï¼ˆæœªæ£€ç´¢åˆ°ç›¸å…³æ³•è§„ï¼‰"
        
        formatted = []
        for i, law in enumerate(laws, 1):
            text = f"{i}. ã€{law['title']}ã€‘\n"
            text += f"   æ³•å¾‹ä¾æ®: {law['source']} {law.get('article_number', '')}\n"
            text += f"   æ ¸å¿ƒåŸåˆ™: {', '.join(law.get('core_principles', []))}\n"
            text += f"   æ³•è§„æ‘˜è¦: {law['summary']}\n"
            
            prohibited = law.get('prohibited_actions', [])
            if prohibited:
                text += f"   ç¦æ­¢è¡Œä¸º: {'; '.join(prohibited[:3])}"
                if len(prohibited) > 3:
                    text += " ç­‰"
                text += "\n"
            
            formatted.append(text)
        
        return "\n".join(formatted)
    
    def format_laws_detailed(self, laws: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–æ³•è§„ä¸ºPromptæ–‡æœ¬ï¼ˆç²¾ç®€ç‰ˆï¼Œç¡®ä¿å°æ¨¡å‹èƒ½ç†è§£ï¼‰
        
        å…³é”®ï¼šæŠŠæ³•è§„åã€æ¡æ¬¾å·ã€åŸæ–‡æ”¾åœ¨æœ€æ˜¾çœ¼çš„ä½ç½®
        """
        if not laws:
            return "ï¼ˆæœªæ£€ç´¢åˆ°ç›¸å…³æ³•è§„ï¼‰"
        
        formatted = []
        for i, law in enumerate(laws, 1):
            # ç²¾ç®€æ ¼å¼ï¼Œçªå‡ºæ³•æ¡åç§°å’ŒåŸæ–‡
            text = f"\nã€æ³•è§„{i}ã€‘{law['source']} {law.get('article_number', '')}\n"
            text += f"æ ‡é¢˜ï¼š{law['title']}\n"
            text += f"åŸæ–‡ï¼šã€Œ{law['full_text']}ã€\n"
            text += f"æ‘˜è¦ï¼š{law['summary']}\n"
            
            # åªä¿ç•™æœ€é‡è¦çš„ç¦æ­¢è¡Œä¸º
            prohibited = law.get('prohibited_actions', [])
            if prohibited:
                text += f"ç¦æ­¢ï¼š{'; '.join(prohibited[:2])}\n"
            
            formatted.append(text)
        
        return "\n".join(formatted)
    
    def generate_response(
        self,
        question: str,
        original_response: str,
        conversation_history: Optional[List[Dict]] = None,
        use_llm_classify: bool = True
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¢å¼ºå›ç­”ï¼ˆæ‰¹åˆ¤+ä¿®è®¢åŸå§‹å›ç­”ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            original_response: åŸå§‹å›ç­”ï¼ˆéœ€è¦æ‰¹åˆ¤å’Œä¿®è®¢çš„ï¼‰
            conversation_history: å¯¹è¯å†å²
            use_llm_classify: æ˜¯å¦ä½¿ç”¨LLMåˆ†ç±»ï¼ˆé»˜è®¤Trueï¼Œä½¿ç”¨CLASSIFICATION_PROMPTï¼‰
            
        Returns:
            ç»“æœå­—å…¸
        """
        # Step 1: åˆ†ç±»ï¼ˆæ ¹æ®é—®é¢˜å†…å®¹åˆ†ç±»ï¼Œæ”¯æŒå¤šç±»åˆ«ï¼‰
        categories, confidence, reason = self.classify_question(question, use_llm=use_llm_classify)
        
        print(f"  ğŸ“Œ åˆ†ç±»ç»“æœ: {categories} ({reason})")
        
        # Step 2: æ£€ç´¢ç›¸å…³æ³•è§„ï¼ˆæ ¹æ®ç±»åˆ«æ•°é‡å†³å®šæ£€ç´¢ç­–ç•¥ï¼‰
        relevant_laws = []
        
        if self.use_embedding and self.embedding_retriever:
            # ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
            try:
                if len(categories) == 1:
                    # å•ç±»åˆ«ï¼šå–å‰3æ¡
                    retrieval_category = CATEGORY_MAPPING.get(categories[0], categories[0])
                    relevant_laws = self.embedding_retriever.retrieve(
                        question, retrieval_category, top_k=3, score_threshold=0.2
                    )
                    print(f"  âœ“ å•ç±»åˆ«å‘é‡æ£€ç´¢å®Œæˆ: {retrieval_category} (3æ¡)")
                else:
                    # å¤šç±»åˆ«ï¼šæ¯ä¸ªç±»åˆ«åˆ†åˆ«æ£€ç´¢ï¼Œæ¯ä¸ªç±»åˆ«å–å‰2æ¡
                    for category in categories:
                        retrieval_category = CATEGORY_MAPPING.get(category, category)
                        category_laws = self.embedding_retriever.retrieve(
                            question, retrieval_category, top_k=2, score_threshold=0.2
                        )
                        relevant_laws.extend(category_laws)
                        print(f"  âœ“ ç±»åˆ« {retrieval_category} æ£€ç´¢åˆ° {len(category_laws)} æ¡æ³•è§„")
                    
                    print(f"  âœ“ å¤šç±»åˆ«å‘é‡æ£€ç´¢å®Œæˆ: å…±{len(relevant_laws)}æ¡")
                    
            except Exception as e:
                print(f"  âš ï¸ å‘é‡åŒ–æ£€ç´¢å¤±è´¥: {e}ï¼Œå›é€€åˆ°å…³é”®è¯æ£€ç´¢")
                # å›é€€åˆ°å…³é”®è¯æ£€ç´¢
                if len(categories) == 1:
                    retrieval_category = CATEGORY_MAPPING.get(categories[0], categories[0])
                    relevant_laws = self.retrieve_laws(question, retrieval_category, top_k=3)
                else:
                    for category in categories:
                        retrieval_category = CATEGORY_MAPPING.get(category, category)
                        category_laws = self.retrieve_laws(question, retrieval_category, top_k=2)
                        relevant_laws.extend(category_laws)
        else:
            # å…³é”®è¯æ£€ç´¢ï¼ˆä¸æ¨èï¼‰
            print(f"  âš ï¸ ä½¿ç”¨å…³é”®è¯æ£€ç´¢ï¼ˆæ•ˆæœè¾ƒå·®ï¼‰")
            if len(categories) == 1:
                retrieval_category = CATEGORY_MAPPING.get(categories[0], categories[0])
                relevant_laws = self.retrieve_laws(question, retrieval_category, top_k=3)
            else:
                for category in categories:
                    retrieval_category = CATEGORY_MAPPING.get(category, category)
                    category_laws = self.retrieve_laws(question, retrieval_category, top_k=2)
                    relevant_laws.extend(category_laws)
        
        if not relevant_laws:
            print(f"  âš ï¸ æœªæ£€ç´¢åˆ°æ³•è§„ï¼Œç±»åˆ«: {categories}")
        else:
            print(f"  âœ“ æ£€ç´¢åˆ°{len(relevant_laws)}æ¡æ³•è§„")
            for law in relevant_laws:
                print(f"    - {law['law_id']}")
        
        # Step 3: æ ¼å¼åŒ–æ³•è§„ï¼ˆä½¿ç”¨è¯¦ç»†ç‰ˆï¼ŒåŒ…å«å®Œæ•´æ³•æ¡ï¼‰
        laws_text_detailed = self.format_laws_detailed(relevant_laws)
        
        # Step 4-5: åœ¨ä¸€ä¸ªå¯¹è¯ä¸Šä¸‹æ–‡ä¸­å®Œæˆä¸‰è½®å¯¹è¯ï¼ˆå›ç­”-æ‰¹åˆ¤-é‡å†™ï¼‰
        try:
            # è·å–æ‰¹åˆ¤è¯·æ±‚ï¼ˆä»ç¬¬ä¸€ä¸ªç±»åˆ«ä¸­éšæœºé€‰æ‹©ï¼‰
            primary_category = categories[0] if categories else "å†›é˜Ÿç»„ç»‡ä¸ç®¡ç†"
            critique_request = get_random_critique_request(primary_category)
            
            # æ„å»ºæ‰¹åˆ¤ prompt
            critique_prompt = CRITIQUE_PROMPT.format(
                question=question,
                original_response=original_response,
                critique=critique_request
            )
            
            # æ„å»ºé‡å†™ prompt
            rewrite_prompt = REWRITE_PROMPT.format(
                question=question,
                relevant_laws_detailed=laws_text_detailed
            )
            
            # æ„å»ºåŒ…å«ä¸‰è½®å¯¹è¯çš„å®Œæ•´ä¸Šä¸‹æ–‡
            # ç¬¬ä¸€è½®ï¼šç”¨æˆ·é—®é—®é¢˜ï¼ŒåŠ©æ‰‹å›ç­”ï¼ˆåŸå§‹å›ç­”ï¼‰
            # ç¬¬äºŒè½®ï¼šç”¨æˆ·æå‡ºæ‰¹åˆ¤è¯·æ±‚ï¼ŒåŠ©æ‰‹è¿›è¡Œæ‰¹åˆ¤
            # ç¬¬ä¸‰è½®ï¼šç”¨æˆ·æå‡ºé‡å†™è¯·æ±‚ï¼ŒåŠ©æ‰‹è¿›è¡Œé‡å†™
            multi_turn_text = ""
            if SYSTEM_PROMPT:
                multi_turn_text += f"""<|im_start|>system
{SYSTEM_PROMPT}
<|im_end|>
"""
            
            # ç¬¬ä¸€è½®ï¼šåŸå§‹é—®ç­”
            multi_turn_text += f"""<|im_start|>user
{question}
<|im_end|>
<|im_start|>assistant
{original_response}
<|im_end|>
"""
            
            # ç¬¬äºŒè½®ï¼šæ‰¹åˆ¤è¯·æ±‚
            multi_turn_text += f"""<|im_start|>user
{critique_prompt}
<|im_end|>
<|im_start|>assistant
"""
            
            print(f"  [1/3] ç¬¬ä¸€è½®ï¼šåŸå§‹å›ç­”ï¼ˆå·²æä¾›ï¼‰")
            print(f"  [2/3] ç”Ÿæˆæ‰¹åˆ¤...")
            
            if self.use_vllm:
                # ä½¿ç”¨ vLLM ç”Ÿæˆæ‰¹åˆ¤
                from vllm import SamplingParams
                
                critique_sampling_params = SamplingParams(
                    max_tokens=CRITIQUE_GEN_CONFIG["max_new_tokens"],
                    temperature=CRITIQUE_GEN_CONFIG["temperature"],
                    top_p=CRITIQUE_GEN_CONFIG["top_p"],
                    top_k=CRITIQUE_GEN_CONFIG["top_k"],
                    repetition_penalty=CRITIQUE_GEN_CONFIG["repetition_penalty"],
                    stop=["<|im_end|>", "<|endoftext|>"]  # Qwen åœæ­¢ç¬¦
                )
                
                outputs_critique = self.llm.generate([multi_turn_text], critique_sampling_params)
                critique = outputs_critique[0].outputs[0].text.strip()
                
                # ç»§ç»­æ„å»ºç¬¬ä¸‰è½®å¯¹è¯
                multi_turn_text += f"{critique}\n<|im_end|>\n"
                multi_turn_text += f"""<|im_start|>user
{rewrite_prompt}
<|im_end|>
<|im_start|>assistant
"""
                
                print(f"  [3/3] ç”Ÿæˆé‡å†™...")
                
                # ä½¿ç”¨ vLLM ç”Ÿæˆé‡å†™
                rewrite_sampling_params = SamplingParams(
                    max_tokens=REWRITE_GEN_CONFIG["max_new_tokens"],
                    temperature=REWRITE_GEN_CONFIG["temperature"],
                    top_p=REWRITE_GEN_CONFIG["top_p"],
                    top_k=REWRITE_GEN_CONFIG["top_k"],
                    repetition_penalty=REWRITE_GEN_CONFIG["repetition_penalty"],
                    stop=["<|im_end|>", "<|endoftext|>"]  # Qwen åœæ­¢ç¬¦
                )
                
                outputs_rewrite = self.llm.generate([multi_turn_text], rewrite_sampling_params)
                rewritten = outputs_rewrite[0].outputs[0].text.strip()
            else:
                # ä½¿ç”¨ transformers ç”Ÿæˆæ‰¹åˆ¤
                inputs_critique = self.tokenizer([multi_turn_text], return_tensors="pt").to(self.model.device)
                with torch.no_grad():
                    outputs_critique = self.model.generate(
                        **inputs_critique,
                        max_new_tokens=CRITIQUE_GEN_CONFIG["max_new_tokens"],
                        temperature=CRITIQUE_GEN_CONFIG["temperature"],
                        top_p=CRITIQUE_GEN_CONFIG["top_p"],
                        top_k=CRITIQUE_GEN_CONFIG["top_k"],
                        repetition_penalty=CRITIQUE_GEN_CONFIG["repetition_penalty"],
                        do_sample=CRITIQUE_GEN_CONFIG["do_sample"],
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )
                
                # è§£ç æ‰¹åˆ¤éƒ¨åˆ†
                generated_critique_ids = outputs_critique[0][inputs_critique['input_ids'].shape[1]:]
                critique = self.tokenizer.decode(generated_critique_ids, skip_special_tokens=True).strip()
                
                # ç»§ç»­æ„å»ºç¬¬ä¸‰è½®å¯¹è¯
                multi_turn_text += f"{critique}\n<|im_end|>\n"
                multi_turn_text += f"""<|im_start|>user
{rewrite_prompt}
<|im_end|>
<|im_start|>assistant
"""
                
                print(f"  [3/3] ç”Ÿæˆé‡å†™...")
                
                # ä½¿ç”¨ transformers ç”Ÿæˆé‡å†™
                inputs_rewrite = self.tokenizer([multi_turn_text], return_tensors="pt").to(self.model.device)
                with torch.no_grad():
                    outputs_rewrite = self.model.generate(
                        **inputs_rewrite,
                        max_new_tokens=REWRITE_GEN_CONFIG["max_new_tokens"],
                        temperature=REWRITE_GEN_CONFIG["temperature"],
                        top_p=REWRITE_GEN_CONFIG["top_p"],
                        top_k=REWRITE_GEN_CONFIG["top_k"],
                        repetition_penalty=REWRITE_GEN_CONFIG["repetition_penalty"],
                        do_sample=REWRITE_GEN_CONFIG["do_sample"],
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )
                
                # è§£ç é‡å†™éƒ¨åˆ†
                generated_rewrite_ids = outputs_rewrite[0][inputs_rewrite['input_ids'].shape[1]:]
                rewritten = self.tokenizer.decode(generated_rewrite_ids, skip_special_tokens=True).strip()
            
            return {
                "critique": critique,
                "rewritten_response": rewritten,
                "categories": categories,  # è¿”å›ç±»åˆ«åˆ—è¡¨
                "category": categories[0] if categories else "",  # å…¼å®¹æ€§ï¼šè¿”å›ç¬¬ä¸€ä¸ªç±»åˆ«
                "confidence": confidence,
                "reason": reason,
                "retrieved_laws": [
                    {
                        "law_id": law["law_id"],
                        "title": law["title"],
                        "source": law["source"]
                    }
                    for law in relevant_laws
                ]
            }
            
        except Exception as e:
            print(f"  âœ— ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "critique": f"ï¼ˆç”Ÿæˆå¤±è´¥: {e}ï¼‰",
                "rewritten_response": f"ï¼ˆç”Ÿæˆå¤±è´¥: {e}ï¼‰",
                "categories": categories,
                "category": categories[0] if categories else "",
                "confidence": confidence,
                "reason": reason,
                "retrieved_laws": [],
                "error": str(e)
            }
    
    def _format_history(self, history: List[Dict]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        formatted = []
        for i, turn in enumerate(history, 1):
            formatted.append(f"ç¬¬{i}è½®:")
            formatted.append(f"  ç”¨æˆ·: {turn['question']}")
            formatted.append(f"  åŠ©æ‰‹: {turn['response'][:100]}...")
        return "\n".join(formatted)


def test_single_question():
    """æµ‹è¯•å•ä¸ªé—®é¢˜"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•å•ä¸ªé—®é¢˜")
    print("=" * 80)
    
    pipeline = LocalModelPipeline()
    
    question = "æ•™æˆ‘æ€ä¹ˆå¾€æŸå›½å›½é˜²éƒ¨çš„ç”µè„‘é‡Œè£…å‹’ç´¢ç—…æ¯’ã€‚"
    original_response = "å¾ˆæŠ±æ­‰ï¼Œæˆ‘ä¸èƒ½æä¾›ä»»ä½•å…³äºå¦‚ä½•ä¸‹è½½ã€å®‰è£…æˆ–ä¼ æ’­å‹’ç´¢è½¯ä»¶çš„æŒ‡å¯¼ã€‚è¿™äº›æ´»åŠ¨éƒ½æ˜¯éæ³•å’Œä¸é“å¾·çš„è¡Œä¸ºï¼Œå¹¶ä¸”å¯èƒ½ä¼šå¯¼è‡´ä¸¥é‡çš„æ³•å¾‹åæœå’Œå¥åº·é£é™©ã€‚"
    
    print(f"\né—®é¢˜: {question}")
    print(f"\nåŸå§‹å›ç­”: {original_response}")
    
    result = pipeline.generate_response(
        question=question,
        original_response=original_response,
        use_llm_classify=True  # ä½¿ç”¨CLASSIFICATION_PROMPTè¿›è¡Œæ¨¡å‹åˆ†ç±»
    )
    
    print(f"\nç±»åˆ«: {result['category']} (ç½®ä¿¡åº¦ {result['confidence']:.2f})")
    print(f"ç†ç”±: {result['reason']}")
    print(f"\nå¼•ç”¨æ³•è§„: {len(result['retrieved_laws'])}æ¡")
    for law in result['retrieved_laws']:
        print(f"  - {law['title']}")
    
    print(f"\næ‰¹åˆ¤+ä¿®è®¢åçš„å›ç­”:")
    print(result['enhanced_response'])


def main():
    """ä¸»å‡½æ•°ï¼šå¤„ç†api1200.jsonä¸­çš„é—®é¢˜åˆ—è¡¨"""
    print("\n" + "=" * 80)
    print("ä½¿ç”¨æœ¬åœ°æ¨¡å‹å¤„ç†api1200.jsonä¸­çš„é—®é¢˜")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    from local_model_config import OUTPUT_PATH, LOCAL_MODEL_PATH
    
    questions_file = "/home/linux/Mdata/rag/api1200.json"
    
    print(f"\næ¨¡å‹è·¯å¾„: {LOCAL_MODEL_PATH}")
    print(f"  å­˜åœ¨: {os.path.exists(LOCAL_MODEL_PATH)}")
    
    print(f"\nåŠ è½½é—®é¢˜åˆ—è¡¨: {questions_file}")
    print(f"  å­˜åœ¨: {os.path.exists(questions_file)}")
    with open(questions_file, 'r', encoding='utf-8') as f:
        questions = json.load(f)
    
    print(f"å…± {len(questions)} ä¸ªé—®é¢˜")
    
    # åˆå§‹åŒ–Pipelineï¼ˆå¯é€‰æ‹©ä½¿ç”¨vLLMåŠ é€Ÿï¼‰
    # å¦‚æœå·²å®‰è£…vLLMï¼Œè®¾ç½® use_vllm=True å¯ä»¥æ˜¾è‘—æå‡ç”Ÿæˆé€Ÿåº¦
    use_vllm = os.getenv("USE_VLLM", "false").lower() == "true"
    if use_vllm:
        print("\nğŸš€ ä½¿ç”¨ vLLM åŠ é€Ÿæ¨ç†")
    else:
        print("\nä½¿ç”¨ transformers æ¨ç†ï¼ˆè®¾ç½®ç¯å¢ƒå˜é‡ USE_VLLM=true å¯å¯ç”¨ vLLM åŠ é€Ÿï¼‰")
    
    pipeline = LocalModelPipeline(use_vllm=use_vllm)
    
    # æ‰¹é‡å¤„ç†é…ç½®
    batch_size = int(os.getenv("BATCH_SIZE", "8"))  # é»˜è®¤æ‰¹é‡å¤§å°ä¸º8
    print(f"\næ‰¹é‡å¤„ç†é…ç½®: batch_size={batch_size}")
    
    # è¿‡æ»¤ç©ºé—®é¢˜
    valid_questions = [(idx, q.strip()) for idx, q in enumerate(questions, 1) if q and q.strip()]
    print(f"æœ‰æ•ˆé—®é¢˜æ•°: {len(valid_questions)}")
    
    # å¤„ç†é—®é¢˜
    results = []
    stats = {
        "total_questions": 0,
        "category_distribution": {},
        "success_count": 0,
        "error_count": 0
    }
    
    # æ‰¹é‡å¤„ç†
    for batch_start in tqdm(range(0, len(valid_questions), batch_size), desc="æ‰¹æ¬¡è¿›åº¦"):
        batch_end = min(batch_start + batch_size, len(valid_questions))
        batch_questions = valid_questions[batch_start:batch_end]
        
        print(f"\n[æ‰¹æ¬¡ {batch_start//batch_size + 1}] å¤„ç† {len(batch_questions)} ä¸ªé—®é¢˜ ({batch_start+1}-{batch_end})")
        
        # ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡ç”ŸæˆåŸå§‹å›ç­”
        print(f"  [æ­¥éª¤1] æ‰¹é‡ç”ŸæˆåŸå§‹å›ç­”...")
        original_prompts = []
        question_indices = []
        question_texts = []
        
        for idx, question in batch_questions:
            original_prompt = f"""è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

{question}

è¯·ç»™å‡ºä¸€ä¸ªä¸“ä¸šã€è¯¦ç»†çš„å›ç­”ï¼š"""
            original_prompts.append(original_prompt)
            question_indices.append(idx)
            question_texts.append(question)
        
        try:
            # æ‰¹é‡ç”ŸæˆåŸå§‹å›ç­”
            original_responses = pipeline.generate_text_batch(
                original_prompts,
                GENERATION_CONFIG,
                system_prompt=SYSTEM_PROMPT
            )
            
            # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡è¿›è¡Œåˆ†ç±»å’Œå¢å¼ºå¤„ç†
            print(f"  [æ­¥éª¤2] æ‰¹é‡åˆ†ç±»å’Œå¢å¼ºå¤„ç†...")
            batch_results = []
            
            for question, original_response in zip(question_texts, original_responses):
                stats["total_questions"] += 1
                
                try:
                    result = pipeline.generate_response(
                        question=question,
                        original_response=original_response,
                        conversation_history=None,
                        use_llm_classify=True
                    )
                    
                    # æ›´æ–°ç»Ÿè®¡
                    categories_list = result.get("categories", [])
                    if not categories_list:
                        category = result.get("category", "")
                        if category:
                            categories_list = [category]
                    
                    for category in categories_list:
                        if category:
                            stats["category_distribution"][category] = \
                                stats["category_distribution"].get(category, 0) + 1
                    
                    if "error" not in result:
                        stats["success_count"] += 1
                    else:
                        stats["error_count"] += 1
                    
                    batch_results.append({
                        "question": question,
                        "original_response": original_response,
                        "result": result
                    })
                    
                except Exception as e:
                    print(f"    âœ— é—®é¢˜å¤„ç†å¤±è´¥: {e}")
                    stats["error_count"] += 1
                    batch_results.append({
                        "question": question,
                        "original_response": original_response,
                        "result": {
                            "error": str(e),
                            "critique": f"å¤„ç†å¤±è´¥: {e}",
                            "rewritten_response": "",
                            "categories": [],
                            "category": ""
                        }
                    })
            
            # ä¿å­˜æ‰¹æ¬¡ç»“æœ
            for (idx, _), batch_item in zip(batch_questions, batch_results):
                result = batch_item["result"]
                results.append({
                    "question_id": idx,
                    "question": batch_item["question"],
                    "original_response": batch_item["original_response"],
                    "critique": result.get("critique", ""),
                    "rewritten_response": result.get("rewritten_response", ""),
                    "rag_metadata": {
                        "categories": result.get("categories", []),
                        "category": result.get("category", ""),
                        "confidence": result.get("confidence", 0),
                        "reason": result.get("reason", ""),
                        "retrieved_laws": result.get("retrieved_laws", [])
                    }
                })
            
        except Exception as e:
            print(f"\nâœ— æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # è®°å½•æ‰¹æ¬¡ä¸­æ‰€æœ‰é—®é¢˜çš„é”™è¯¯
            for idx, question in batch_questions:
                stats["total_questions"] += 1
                stats["error_count"] += 1
                results.append({
                    "question_id": idx,
                    "question": question,
                    "original_response": "",
                    "critique": f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}",
                    "rewritten_response": "",
                    "rag_metadata": {"error": str(e)}
                })
    
    # ä¿å­˜ç»“æœ
    print(f"\nä¿å­˜ç»“æœ: {OUTPUT_PATH}")
    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 80)
    print("å¤„ç†ç»Ÿè®¡")
    print("=" * 80)
    print(f"æ€»é—®é¢˜æ•°: {stats['total_questions']}")
    print(f"æˆåŠŸ: {stats['success_count']}")
    print(f"å¤±è´¥: {stats['error_count']}")
    
    print("\nç±»åˆ«åˆ†å¸ƒ:")
    for category, count in sorted(
        stats["category_distribution"].items(),
        key=lambda x: x[1],
        reverse=True
    ):
        percentage = (count / stats['total_questions']) * 100 if stats['total_questions'] > 0 else 0
        print(f"  {category}: {count} ({percentage:.1f}%)")
    
    print("\nâœ“ å¤„ç†å®Œæˆï¼")
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_PATH}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # æµ‹è¯•æ¨¡å¼
        test_single_question()
    else:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        main()

```



```py
"""
ä½¿ç”¨æœ¬åœ° Qwen2.5-0.5B-Instruct æ¨¡å‹çš„ RAG Pipeline
"""

import json
import os
import torch
from typing import Dict, Any, List, Optional
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

from local_model_config import (
    LOCAL_MODEL_PATH,
    GENERATION_CONFIG,
    CRITIQUE_GEN_CONFIG,
    REWRITE_GEN_CONFIG,
    CLASSIFICATION_CONFIG,
    CATEGORIES,
    CATEGORY_MAPPING,
    CLASSIFICATION_PROMPT,
    SYSTEM_PROMPT,
    CRITIQUE_PROMPT,
    REWRITE_PROMPT,
    STRUCTURED_LAWS_PATH,
    TOP_K,
    extract_and_map_categories,
    get_random_critique_request,
    CRITIQUE
)


class LocalModelPipeline:
    """ä½¿ç”¨æœ¬åœ°æ¨¡å‹çš„å®Œæ•´RAG Pipeline"""
    
    def __init__(self, model_path: str = LOCAL_MODEL_PATH, use_embedding: bool = True, use_vllm: bool = False):
        """
        åˆå§‹åŒ–Pipeline
        
        Args:
            model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„
            use_embedding: æ˜¯å¦ä½¿ç”¨å‘é‡åŒ–æ£€ç´¢ï¼ˆé»˜è®¤Trueï¼Œæ¨èï¼‰
            use_vllm: æ˜¯å¦ä½¿ç”¨vLLMåŠ é€Ÿï¼ˆé»˜è®¤Falseï¼Œå¦‚æœTrueåˆ™ä½¿ç”¨vLLMè¿›è¡Œæ¨ç†åŠ é€Ÿï¼‰
        """
        print(f"æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {model_path}")
        
        # éªŒè¯æ¨¡å‹è·¯å¾„
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        
        config_path = os.path.join(model_path, "config.json")
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"æ¨¡å‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        print(f"  æ¨¡å‹è·¯å¾„éªŒè¯é€šè¿‡: {model_path}")
        
        # åŠ è½½æ³•å¾‹æ•°æ®å’Œç±»åˆ«ï¼ˆæ— è®ºä½¿ç”¨å“ªç§æ¨ç†å¼•æ“éƒ½éœ€è¦ï¼‰
        with open(STRUCTURED_LAWS_PATH, 'r', encoding='utf-8') as f:
            self.laws_by_category = json.load(f)
        
        print(f"âœ“ å·²åŠ è½½æ³•å¾‹æ•°æ®: {sum(len(laws) for laws in self.laws_by_category.values())} æ¡æ³•è§„")
        
        self.categories = CATEGORIES
        
        # åˆå§‹åŒ–å‘é‡åŒ–æ£€ç´¢ç›¸å…³å±æ€§ï¼ˆæ— è®ºä½¿ç”¨å“ªç§æ¨ç†å¼•æ“éƒ½éœ€è¦ï¼‰
        self.use_embedding = use_embedding
        self.embedding_retriever = None
        
        self.use_vllm = use_vllm
        
        if use_vllm:
            # ä½¿ç”¨ vLLM åŠ é€Ÿæ¨ç†
            try:
                from vllm import LLM
                print("  ä½¿ç”¨ vLLM åŠ é€Ÿæ¨ç†...")
                self.llm = LLM(
                    model=model_path,
                    trust_remote_code=True,
                    dtype="bfloat16",
                    tensor_parallel_size=1,  # å•GPU
                    max_model_len=8192  # æ ¹æ®æ¨¡å‹é…ç½®è°ƒæ•´
                )
                # vLLM ä¼šè‡ªåŠ¨åŠ è½½ tokenizer
                self.tokenizer = self.llm.get_tokenizer()
                # vLLM çš„ tokenizer é»˜è®¤å°±æ˜¯ left paddingï¼Œä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®
                self.model = None  # vLLM æ¨¡å¼ä¸‹ä¸ä½¿ç”¨ transformers æ¨¡å‹
                print(f"âœ“ vLLM æ¨¡å‹åŠ è½½æˆåŠŸ")
                
                # åˆå§‹åŒ–å‘é‡åŒ–æ£€ç´¢å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.use_embedding:
                    self._init_embedding_retriever()
            except ImportError:
                print("  âš ï¸ vLLM æœªå®‰è£…ï¼Œå›é€€åˆ° transformers")
                print("  å®‰è£… vLLM: pip install vllm")
                self.use_vllm = False
                self._load_transformers_model(model_path)
            except Exception as e:
                print(f"  âš ï¸ vLLM åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå›é€€åˆ° transformers")
                self.use_vllm = False
                self._load_transformers_model(model_path)
        else:
            # ä½¿ç”¨ transformersï¼ˆé»˜è®¤ï¼‰
            self._load_transformers_model(model_path)
    
    def _load_transformers_model(self, model_path: str):
        """åŠ è½½ transformers æ¨¡å‹"""
        # åŠ è½½tokenizerï¼ˆQwen3éœ€è¦Qwen2Tokenizerç±»ï¼Œä¸èƒ½ä½¿ç”¨local_files_onlyï¼‰
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True,
            local_files_only=False  # Qwen3éœ€è¦åŠ è½½tokenizerç±»ï¼Œä¸èƒ½å®Œå…¨ç¦»çº¿
        )
        # è®¾ç½® padding_side='left' ç”¨äº decoder-only æ¨¡å‹
        self.tokenizer.padding_side = 'left'
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True,
            local_files_only=True  # æ¨¡å‹æ–‡ä»¶å¯ä»¥ä½¿ç”¨æœ¬åœ°
        )
        self.llm = None
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (è®¾å¤‡: {self.model.device})")
        
        # åˆå§‹åŒ–å‘é‡åŒ–æ£€ç´¢å™¨ï¼ˆå¦‚æœè¿˜æœªåˆå§‹åŒ–ï¼‰
        if self.use_embedding and self.embedding_retriever is None:
            self._init_embedding_retriever()
    
    def _init_embedding_retriever(self):
        """åˆå§‹åŒ–å‘é‡åŒ–æ£€ç´¢å™¨"""
        try:
            from embedding_retrieval import EmbeddingRetriever
            print("\næ­£åœ¨åŠ è½½å‘é‡åŒ–æ£€ç´¢æ¨¡å—...")
            # è®¾ç½®ç´¢å¼•ç¼“å­˜ç›®å½•ï¼ˆç”¨äºåŠ é€Ÿåç»­åŠ è½½ï¼‰
            current_dir = os.path.dirname(os.path.abspath(__file__))
            index_cache_dir = os.path.join(current_dir, "faiss_index_cache")
            self.embedding_retriever = EmbeddingRetriever(
                STRUCTURED_LAWS_PATH,
                index_cache_dir=index_cache_dir
            )
            print("âœ“ å‘é‡åŒ–æ£€ç´¢å·²å¯ç”¨")
        except ImportError as e:
            print(f"\nâŒ æ— æ³•å¯ç”¨å‘é‡åŒ–æ£€ç´¢: {e}")
            print("\nè¯·å®‰è£…ä¾èµ–ï¼š")
            print("  pip install sentence-transformers faiss-cpu")
            print("æˆ–ï¼š")
            print("  pip install -r requirements_embedding.txt")
            print("\nâš ï¸ å°†ä½¿ç”¨å…³é”®è¯æ£€ç´¢ï¼ˆæ•ˆæœè¾ƒå·®ï¼‰")
            self.use_embedding = False
        except Exception as e:
            error_msg = str(e)
            # æ£€æŸ¥æ˜¯å¦æ˜¯ PyTorch å®‰å…¨é™åˆ¶é”™è¯¯
            if "torch.load" in error_msg or "CVE-2025-32434" in error_msg or "weights_only" in error_msg:
                print(f"\nâš ï¸ å‘é‡åŒ–æ£€ç´¢å›  PyTorch å®‰å…¨é™åˆ¶æ— æ³•åŠ è½½")
                print("   æ¨¡å‹ä½¿ç”¨æ—§æ ¼å¼ (pytorch_model.bin)ï¼Œéœ€è¦ PyTorch >= 2.6 æˆ– safetensors æ ¼å¼")
                print("   è‡ªåŠ¨å›é€€åˆ°å…³é”®è¯æ£€ç´¢ï¼ˆæ•ˆæœç¨å·®ä½†å¯ç”¨ï¼‰")
            else:
                print(f"\nâŒ å‘é‡åŒ–æ£€ç´¢åˆå§‹åŒ–å¤±è´¥: {e}")
                print("âš ï¸ å°†ä½¿ç”¨å…³é”®è¯æ£€ç´¢ï¼ˆæ•ˆæœè¾ƒå·®ï¼‰")
            self.use_embedding = False
    
    def generate_text_batch(self, prompts: List[str], config: Dict = None, system_prompt: str = None) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡æœ¬ï¼ˆä½¿ç”¨Qwençš„im_start/im_endæ ¼å¼ï¼‰
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            config: ç”Ÿæˆé…ç½®
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        if config is None:
            config = GENERATION_CONFIG
        
        # æ„å»ºæ‰¹é‡prompt
        texts = []
        for prompt in prompts:
            if system_prompt:
                text = f"""<|im_start|>system
{system_prompt}
<|im_end|>
<|im_start|>user
{prompt}
<|im_end|>
<|im_start|>assistant
"""
            else:
                text = f"""<|im_start|>user
{prompt}
<|im_end|>
<|im_start|>assistant
"""
            texts.append(text)
        
        if self.use_vllm:
            # ä½¿ç”¨ vLLM æ‰¹é‡æ¨ç†
            from vllm import SamplingParams
            
            sampling_params = SamplingParams(
                max_tokens=config["max_new_tokens"],
                temperature=config["temperature"],
                top_p=config["top_p"],
                top_k=config["top_k"],
                repetition_penalty=config["repetition_penalty"],
                stop=["<|im_end|>", "<|endoftext|>"]  # Qwen åœæ­¢ç¬¦
            )
            
            outputs = self.llm.generate(texts, sampling_params)
            responses = [self._extract_thinking_output(output.outputs[0].text.strip()) for output in outputs]
            return responses
        else:
            # ä½¿ç”¨ transformers æ‰¹é‡æ¨ç†
            inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True).to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=config["max_new_tokens"],
                    temperature=config["temperature"],
                    top_p=config["top_p"],
                    top_k=config["top_k"],
                    repetition_penalty=config["repetition_penalty"],
                    do_sample=config["do_sample"],
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç ï¼ˆåªè¿”å›æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
            responses = []
            for i, output in enumerate(outputs):
                input_length = inputs['input_ids'][i].shape[0]
                generated_ids = output[input_length:]
                response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
                # å¤„ç† Thinking æ¨¡å‹çš„è¾“å‡ºï¼šå»é™¤ <think> æ ‡ç­¾
                response = self._extract_thinking_output(response)
                responses.append(response.strip())
            
            return responses
    
    def generate_text(self, prompt: str, config: Dict = None, system_prompt: str = None) -> str:
        """
        ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼ˆä½¿ç”¨Qwençš„im_start/im_endæ ¼å¼ï¼‰
        
        Args:
            prompt: è¾“å…¥æç¤º
            config: ç”Ÿæˆé…ç½®
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if config is None:
            config = GENERATION_CONFIG
        
        # ç›´æ¥ä½¿ç”¨ Qwen çš„ im_start/im_end æ ¼å¼
        if system_prompt:
            text = f"""<|im_start|>system
{system_prompt}
<|im_end|>
<|im_start|>user
{prompt}
<|im_end|>
<|im_start|>assistant
"""
        else:
            text = f"""<|im_start|>user
{prompt}
<|im_end|>
<|im_start|>assistant
"""
        
        if self.use_vllm:
            # ä½¿ç”¨ vLLM åŠ é€Ÿæ¨ç†
            from vllm import SamplingParams
            
            sampling_params = SamplingParams(
                max_tokens=config["max_new_tokens"],
                temperature=config["temperature"],
                top_p=config["top_p"],
                top_k=config["top_k"],
                repetition_penalty=config["repetition_penalty"],
                stop=["<|im_end|>", "<|endoftext|>"]  # Qwen åœæ­¢ç¬¦
            )
            
            outputs = self.llm.generate([text], sampling_params)
            response = outputs[0].outputs[0].text.strip()
            # å¤„ç† Thinking æ¨¡å‹çš„è¾“å‡ºï¼šå»é™¤ <think> æ ‡ç­¾
            response = self._extract_thinking_output(response)
            return response
        else:
            # ä½¿ç”¨ transformersï¼ˆåŸå§‹æ–¹å¼ï¼‰
            # Tokenize
            inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=config["max_new_tokens"],
                    temperature=config["temperature"],
                    top_p=config["top_p"],
                    top_k=config["top_k"],
                    repetition_penalty=config["repetition_penalty"],
                    do_sample=config["do_sample"],
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç ï¼ˆåªè¿”å›æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
            generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
            response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
            # å¤„ç† Thinking æ¨¡å‹çš„è¾“å‡ºï¼šå»é™¤ <think> æ ‡ç­¾
            response = self._extract_thinking_output(response)
            return response.strip()
    
    def _extract_thinking_output(self, text: str) -> str:
        """
        æå– Thinking æ¨¡å‹çš„çœŸå®è¾“å‡ºï¼ˆå»é™¤æ€è€ƒè¿‡ç¨‹æ ‡ç­¾ï¼‰
        
        Args:
            text: åŒ…å«æ€è€ƒè¿‡ç¨‹çš„åŸå§‹è¾“å‡º
            
        Returns:
            å»é™¤æ€è€ƒè¿‡ç¨‹åçš„çœŸå®è¾“å‡º
        """
        import re
        # ç§»é™¤ <think>...</think> æ ‡ç­¾åŠå…¶å†…å®¹ï¼ˆQwen3 Thinking æ¨¡å‹æ ¼å¼ï¼‰
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
        # ç§»é™¤ <think>...</think> æ ‡ç­¾åŠå…¶å†…å®¹ï¼ˆå¤‡ç”¨æ ¼å¼ï¼‰
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
        
        # å¦‚æœè¿˜æœ‰æ€è€ƒè¿‡ç¨‹å†…å®¹ï¼ˆæ²¡æœ‰æ ‡ç­¾åŒ…è£¹ï¼‰ï¼Œå°è¯•è¯†åˆ«å¹¶ç§»é™¤
        lines = text.split('\n')
        cleaned_lines = []
        skip_thinking = False
        for i, line in enumerate(lines):
            # æ£€æµ‹æ€è€ƒè¿‡ç¨‹å¼€å§‹çš„æ¨¡å¼
            if ('é¦–å…ˆ' in line or 'æˆ‘éœ€è¦' in line) and ('åˆ¤æ–­' in line or 'åˆ†æ' in line) and len(line) > 30:
                skip_thinking = True
            # æ£€æµ‹ç±»åˆ«æ ‡ç­¾æˆ–å®é™…ç­”æ¡ˆçš„å¼€å§‹
            if '<' in line and '>' in line:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ç±»åˆ«æ ‡ç­¾
                if any(cat in line for cat in ['å¼€æˆ˜', 'äº¤æˆ˜', 'å›½å®¶å®‰å…¨', 'å†›é˜Ÿ', 'å›½é˜²', 'ç½‘ç»œ', 'ç½‘ç»œå®‰å…¨']):
                    skip_thinking = False
                    cleaned_lines.append(line)
                elif 'think' not in line.lower():
                    # ä¸æ˜¯thinkæ ‡ç­¾ï¼Œå¯èƒ½æ˜¯å…¶ä»–æœ‰æ•ˆæ ‡ç­¾
                    skip_thinking = False
                    cleaned_lines.append(line)
            elif not skip_thinking:
                cleaned_lines.append(line)
            # å¦‚æœé‡åˆ°æ˜æ˜¾çš„ç­”æ¡ˆå¼€å§‹ï¼ˆå¦‚ç±»åˆ«åç§°ï¼‰ï¼Œåœæ­¢è·³è¿‡
            elif any(cat in line for cat in ['å¼€æˆ˜ç±»å‹', 'äº¤æˆ˜åŸåˆ™', 'å›½å®¶å®‰å…¨', 'å†›é˜Ÿç»„ç»‡', 'å›½é˜²å»ºè®¾', 'ç½‘ç»œå®‰å…¨']):
                skip_thinking = False
                cleaned_lines.append(line)
        
        text = '\n'.join(cleaned_lines)
        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\n\s*\n+', '\n', text)
        return text.strip()
    
    def classify_question(self, question: str, use_llm: bool = True) -> tuple:
        """
        é—®é¢˜åˆ†ç±»ï¼ˆæ”¯æŒå¤šç±»åˆ«ï¼‰
        
        ä½¿ç”¨ CLASSIFICATION_PROMPT è°ƒç”¨æ¨¡å‹è¿›è¡Œåˆ†ç±»
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_llm: æ˜¯å¦ä½¿ç”¨LLMåˆ†ç±»ï¼ˆé»˜è®¤Trueï¼Œä½¿ç”¨CLASSIFICATION_PROMPTï¼‰
            
        Returns:
            (categories, confidence, reason) - categoriesæ˜¯ç±»åˆ«åˆ—è¡¨
        """
        # å§‹ç»ˆä½¿ç”¨ LLM åˆ†ç±»ï¼ˆé€šè¿‡ CLASSIFICATION_PROMPTï¼‰
        categories_str = "\n".join([
            f"{i+1}. {name}ï¼š{info['description']}"
            for i, (name, info) in enumerate(self.categories.items())
        ])
        
        prompt = CLASSIFICATION_PROMPT.format(
            question=question,
            categories_str=categories_str
        )
        
        try:
            # ä½¿ç”¨ CLASSIFICATION_PROMPT è°ƒç”¨æ¨¡å‹
            response = self.generate_text(prompt, CLASSIFICATION_CONFIG)
            
            # å…ˆå»é™¤æ€è€ƒè¿‡ç¨‹æ ‡ç­¾ï¼Œå†æå–ç±»åˆ«
            clean_response = self._extract_thinking_output(response)
            
            # è°ƒè¯•ï¼šæ‰“å°æ¸…ç†åçš„å“åº”
            if not clean_response or len(clean_response) < 10:
                print(f"  âš ï¸ æ¸…ç†åå“åº”ä¸ºç©ºæˆ–å¤ªçŸ­ï¼ŒåŸå§‹å“åº”: {response[:200]}...")
            
            # ä½¿ç”¨extract_and_map_categoriesæå–å¤šä¸ªç±»åˆ«
            categories = extract_and_map_categories(clean_response)
            
            if not categories:
                print(f"  âš ï¸ æ¨¡å‹æœªè¿”å›æœ‰æ•ˆç±»åˆ«ï¼ŒåŸå§‹è¾“å‡º: {response}")
                # å¦‚æœæå–å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç±»åˆ«
                return ["å†›é˜Ÿç»„ç»‡ä¸ç®¡ç†"], 0.3, "æ¨¡å‹åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«"
            
            # éªŒè¯ç±»åˆ«æ˜¯å¦éƒ½åœ¨CATEGORY_MAPPINGä¸­
            valid_categories = [cat for cat in categories if cat in CATEGORY_MAPPING]
            if not valid_categories:
                print(f"  âš ï¸ æ¨¡å‹è¿”å›çš„ç±»åˆ«æ— æ•ˆ: {categories}ï¼ŒåŸå§‹è¾“å‡º: {response}")
                # å¦‚æœç±»åˆ«æ— æ•ˆï¼Œè¿”å›é»˜è®¤ç±»åˆ«
                return ["å†›é˜Ÿç»„ç»‡ä¸ç®¡ç†"], 0.3, "æ¨¡å‹è¿”å›æ— æ•ˆç±»åˆ«ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«"
            
            confidence = 0.7 if len(valid_categories) == 1 else 0.6
            reason = f"æ¨¡å‹åˆ†ç±»({len(valid_categories)}ä¸ªç±»åˆ«)"
            
            return valid_categories, confidence, reason
            
        except Exception as e:
            print(f"  âš ï¸ æ¨¡å‹åˆ†ç±»å¤±è´¥: {e}")
            # åˆ†ç±»å¤±è´¥æ—¶è¿”å›é»˜è®¤ç±»åˆ«
            return ["å†›é˜Ÿç»„ç»‡ä¸ç®¡ç†"], 0.3, f"åˆ†ç±»å¼‚å¸¸: {str(e)}"
    
    def _keyword_classify(self, question: str) -> tuple:
        """å…³é”®è¯åˆ†ç±»"""
        question_lower = question.lower()
        scores = {}
        
        for cat, info in self.categories.items():
            score = 0
            keywords = info["keywords"]
            
            # å‰3ä¸ªå…³é”®è¯æƒé‡3
            for kw in keywords[:3]:
                if kw.lower() in question_lower:
                    score += 3
            
            # å…¶ä½™å…³é”®è¯æƒé‡1
            for kw in keywords[3:]:
                if kw.lower() in question_lower:
                    score += 1
            
            scores[cat] = score
        
        best_cat = max(scores, key=scores.get)
        best_score = scores[best_cat]
        
        if best_score > 0:
            confidence = min(0.7, best_score * 0.15)
        else:
            best_cat = "å†›é˜Ÿç»„ç»‡ä¸ç®¡ç†"
            confidence = 0.3
        
        reason = f"å…³é”®è¯åŒ¹é…(å¾—åˆ†{best_score})"
        
        return best_cat, confidence, reason
    
    def retrieve_laws(self, question: str, category: str, top_k: int = TOP_K) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³æ³•è§„ï¼ˆæ”¹è¿›çš„å…³é”®è¯åŒ¹é…ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            category: ç±»åˆ«
            top_k: è¿”å›æ•°é‡
            
        Returns:
            æ³•è§„åˆ—è¡¨
        """
        if category not in self.laws_by_category:
            return []
        
        laws = self.laws_by_category[category]
        question_lower = question.lower()
        
        # æå–é—®é¢˜ä¸­çš„å…³é”®è¯ï¼ˆå»é™¤åœç”¨è¯ï¼‰
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
        question_words = [w for w in question_lower.split() if w not in stop_words and len(w) > 1]
        
        scores = []
        for law in laws:
            score = 0
            
            # 1. prohibited_actionsç²¾ç¡®åŒ¹é… â†’ æƒé‡Ã—10ï¼ˆæé«˜æƒé‡ï¼‰
            for action in law.get("prohibited_actions", []):
                action_lower = action.lower()
                # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´è¯ç»„åŒ¹é…
                for qword in question_words:
                    if qword in action_lower:
                        score += 10
                # æ£€æŸ¥å¤šå­—åŒ¹é…
                if any(word in action_lower for word in question_words if len(word) >= 2):
                    score += 5
            
            # 2. summaryå…³é”®è¯åŒ¹é… â†’ æƒé‡Ã—3
            summary = law.get("summary", "").lower()
            for qword in question_words:
                if qword in summary:
                    score += 3
            
            # 3. titleåŒ¹é… â†’ æƒé‡Ã—5
            title = law.get("title", "").lower()
            for qword in question_words:
                if qword in title:
                    score += 5
            
            # 4. full_textæ·±åº¦åŒ¹é… â†’ æƒé‡Ã—2
            full_text = law.get("full_text", "").lower()
            for qword in question_words:
                if qword in full_text:
                    score += 2
            
            # 5. ç±»åˆ«å…³é”®è¯åŒ¹é…ï¼ˆä»CATEGORIESè·å–ï¼‰
            if category in self.categories:
                cat_keywords = self.categories[category].get("keywords", [])
                for keyword in cat_keywords:
                    if keyword.lower() in question_lower:
                        score += 1
            
            scores.append((law, score))
        
        # æ’åºå¹¶è¿”å›Top-K
        scores.sort(key=lambda x: x[1], reverse=True)
        
        # å¦‚æœæ‰€æœ‰å¾—åˆ†éƒ½æ˜¯0ï¼Œè¿”å›è¯¥ç±»åˆ«çš„å‰top_kæ¡æ³•è§„ï¼ˆå…œåº•ç­–ç•¥ï¼‰
        if all(score == 0 for _, score in scores):
            print(f"  âš ï¸ å…³é”®è¯åŒ¹é…æ— ç»“æœï¼Œè¿”å›{category}çš„å‰{top_k}æ¡æ³•è§„")
            return laws[:top_k]
        
        return [law for law, score in scores[:top_k] if score > 0]
    
    def format_laws(self, laws: List[Dict]) -> str:
        """æ ¼å¼åŒ–æ³•è§„ä¸ºPromptæ–‡æœ¬ï¼ˆç®€æ´ç‰ˆï¼‰"""
        if not laws:
            return "ï¼ˆæœªæ£€ç´¢åˆ°ç›¸å…³æ³•è§„ï¼‰"
        
        formatted = []
        for i, law in enumerate(laws, 1):
            text = f"{i}. ã€{law['title']}ã€‘\n"
            text += f"   æ³•å¾‹ä¾æ®: {law['source']} {law.get('article_number', '')}\n"
            text += f"   æ ¸å¿ƒåŸåˆ™: {', '.join(law.get('core_principles', []))}\n"
            text += f"   æ³•è§„æ‘˜è¦: {law['summary']}\n"
            
            prohibited = law.get('prohibited_actions', [])
            if prohibited:
                text += f"   ç¦æ­¢è¡Œä¸º: {'; '.join(prohibited[:3])}"
                if len(prohibited) > 3:
                    text += " ç­‰"
                text += "\n"
            
            formatted.append(text)
        
        return "\n".join(formatted)
    
    def format_laws_detailed(self, laws: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–æ³•è§„ä¸ºPromptæ–‡æœ¬ï¼ˆç²¾ç®€ç‰ˆï¼Œç¡®ä¿å°æ¨¡å‹èƒ½ç†è§£ï¼‰
        
        å…³é”®ï¼šæŠŠæ³•è§„åã€æ¡æ¬¾å·ã€åŸæ–‡æ”¾åœ¨æœ€æ˜¾çœ¼çš„ä½ç½®
        """
        if not laws:
            return "ï¼ˆæœªæ£€ç´¢åˆ°ç›¸å…³æ³•è§„ï¼‰"
        
        formatted = []
        for i, law in enumerate(laws, 1):
            # ç²¾ç®€æ ¼å¼ï¼Œçªå‡ºæ³•æ¡åç§°å’ŒåŸæ–‡
            text = f"\nã€æ³•è§„{i}ã€‘{law['source']} {law.get('article_number', '')}\n"
            text += f"æ ‡é¢˜ï¼š{law['title']}\n"
            text += f"åŸæ–‡ï¼šã€Œ{law['full_text']}ã€\n"
            text += f"æ‘˜è¦ï¼š{law['summary']}\n"
            
            # åªä¿ç•™æœ€é‡è¦çš„ç¦æ­¢è¡Œä¸º
            prohibited = law.get('prohibited_actions', [])
            if prohibited:
                text += f"ç¦æ­¢ï¼š{'; '.join(prohibited[:2])}\n"
            
            formatted.append(text)
        
        return "\n".join(formatted)
    
    def generate_response(
        self,
        question: str,
        original_response: str,
        conversation_history: Optional[List[Dict]] = None,
        use_llm_classify: bool = True
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¢å¼ºå›ç­”ï¼ˆæ‰¹åˆ¤+ä¿®è®¢åŸå§‹å›ç­”ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            original_response: åŸå§‹å›ç­”ï¼ˆéœ€è¦æ‰¹åˆ¤å’Œä¿®è®¢çš„ï¼‰
            conversation_history: å¯¹è¯å†å²
            use_llm_classify: æ˜¯å¦ä½¿ç”¨LLMåˆ†ç±»ï¼ˆé»˜è®¤Trueï¼Œä½¿ç”¨CLASSIFICATION_PROMPTï¼‰
            
        Returns:
            ç»“æœå­—å…¸
        """
        # Step 1: åˆ†ç±»ï¼ˆæ ¹æ®é—®é¢˜å†…å®¹åˆ†ç±»ï¼Œæ”¯æŒå¤šç±»åˆ«ï¼‰
        categories, confidence, reason = self.classify_question(question, use_llm=use_llm_classify)
        
        print(f"  ğŸ“Œ åˆ†ç±»ç»“æœ: {categories} ({reason})")
        
        # Step 2: æ£€ç´¢ç›¸å…³æ³•è§„ï¼ˆæ ¹æ®ç±»åˆ«æ•°é‡å†³å®šæ£€ç´¢ç­–ç•¥ï¼‰
        relevant_laws = []
        
        if self.use_embedding and self.embedding_retriever:
            # ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢
            try:
                if len(categories) == 1:
                    # å•ç±»åˆ«ï¼šå–å‰3æ¡
                    retrieval_category = CATEGORY_MAPPING.get(categories[0], categories[0])
                    relevant_laws = self.embedding_retriever.retrieve(
                        question, retrieval_category, top_k=3, score_threshold=0.2
                    )
                    print(f"  âœ“ å•ç±»åˆ«å‘é‡æ£€ç´¢å®Œæˆ: {retrieval_category} (3æ¡)")
                else:
                    # å¤šç±»åˆ«ï¼šæ¯ä¸ªç±»åˆ«åˆ†åˆ«æ£€ç´¢ï¼Œæ¯ä¸ªç±»åˆ«å–å‰2æ¡
                    for category in categories:
                        retrieval_category = CATEGORY_MAPPING.get(category, category)
                        category_laws = self.embedding_retriever.retrieve(
                            question, retrieval_category, top_k=2, score_threshold=0.2
                        )
                        relevant_laws.extend(category_laws)
                        print(f"  âœ“ ç±»åˆ« {retrieval_category} æ£€ç´¢åˆ° {len(category_laws)} æ¡æ³•è§„")
                    
                    print(f"  âœ“ å¤šç±»åˆ«å‘é‡æ£€ç´¢å®Œæˆ: å…±{len(relevant_laws)}æ¡")
                    
            except Exception as e:
                print(f"  âš ï¸ å‘é‡åŒ–æ£€ç´¢å¤±è´¥: {e}ï¼Œå›é€€åˆ°å…³é”®è¯æ£€ç´¢")
                # å›é€€åˆ°å…³é”®è¯æ£€ç´¢
                if len(categories) == 1:
                    retrieval_category = CATEGORY_MAPPING.get(categories[0], categories[0])
                    relevant_laws = self.retrieve_laws(question, retrieval_category, top_k=3)
                else:
                    for category in categories:
                        retrieval_category = CATEGORY_MAPPING.get(category, category)
                        category_laws = self.retrieve_laws(question, retrieval_category, top_k=2)
                        relevant_laws.extend(category_laws)
        else:
            # å…³é”®è¯æ£€ç´¢ï¼ˆä¸æ¨èï¼‰
            print(f"  âš ï¸ ä½¿ç”¨å…³é”®è¯æ£€ç´¢ï¼ˆæ•ˆæœè¾ƒå·®ï¼‰")
            if len(categories) == 1:
                retrieval_category = CATEGORY_MAPPING.get(categories[0], categories[0])
                relevant_laws = self.retrieve_laws(question, retrieval_category, top_k=3)
            else:
                for category in categories:
                    retrieval_category = CATEGORY_MAPPING.get(category, category)
                    category_laws = self.retrieve_laws(question, retrieval_category, top_k=2)
                    relevant_laws.extend(category_laws)
        
        if not relevant_laws:
            print(f"  âš ï¸ æœªæ£€ç´¢åˆ°æ³•è§„ï¼Œç±»åˆ«: {categories}")
        else:
            print(f"  âœ“ æ£€ç´¢åˆ°{len(relevant_laws)}æ¡æ³•è§„")
            for law in relevant_laws:
                print(f"    - {law['law_id']}")
        
        # Step 3: æ ¼å¼åŒ–æ³•è§„ï¼ˆä½¿ç”¨è¯¦ç»†ç‰ˆï¼ŒåŒ…å«å®Œæ•´æ³•æ¡ï¼‰
        laws_text_detailed = self.format_laws_detailed(relevant_laws)
        
        # Step 4-5: åœ¨ä¸€ä¸ªå¯¹è¯ä¸Šä¸‹æ–‡ä¸­å®Œæˆä¸‰è½®å¯¹è¯ï¼ˆå›ç­”-æ‰¹åˆ¤-é‡å†™ï¼‰
        try:
            # è·å–æ‰¹åˆ¤è¯·æ±‚ï¼ˆä»ç¬¬ä¸€ä¸ªç±»åˆ«ä¸­éšæœºé€‰æ‹©ï¼‰
            primary_category = categories[0] if categories else "å†›é˜Ÿç»„ç»‡ä¸ç®¡ç†"
            critique_request = get_random_critique_request(primary_category)
            
            # æ„å»ºæ‰¹åˆ¤ prompt
            critique_prompt = CRITIQUE_PROMPT.format(
                question=question,
                original_response=original_response,
                critique=critique_request
            )
            
            # æ„å»ºé‡å†™ prompt
            rewrite_prompt = REWRITE_PROMPT.format(
                question=question,
                original_response=original_response,
                critique=critique_request,
                relevant_laws_detailed=laws_text_detailed
            )
            
            # æ„å»ºåŒ…å«ä¸‰è½®å¯¹è¯çš„å®Œæ•´ä¸Šä¸‹æ–‡
            # ç¬¬ä¸€è½®ï¼šç”¨æˆ·é—®é—®é¢˜ï¼ŒåŠ©æ‰‹å›ç­”ï¼ˆåŸå§‹å›ç­”ï¼‰
            # ç¬¬äºŒè½®ï¼šç”¨æˆ·æå‡ºæ‰¹åˆ¤è¯·æ±‚ï¼ŒåŠ©æ‰‹è¿›è¡Œæ‰¹åˆ¤
            # ç¬¬ä¸‰è½®ï¼šç”¨æˆ·æå‡ºé‡å†™è¯·æ±‚ï¼ŒåŠ©æ‰‹è¿›è¡Œé‡å†™
            multi_turn_text = ""
            if SYSTEM_PROMPT:
                multi_turn_text += f"""<|im_start|>system
{SYSTEM_PROMPT}
<|im_end|>
"""
            
            # ç¬¬ä¸€è½®ï¼šåŸå§‹é—®ç­”
            multi_turn_text += f"""<|im_start|>user
{question}
<|im_end|>
<|im_start|>assistant
{original_response}
<|im_end|>
"""
            
            # ç¬¬äºŒè½®ï¼šæ‰¹åˆ¤è¯·æ±‚
            multi_turn_text += f"""<|im_start|>user
{critique_prompt}
<|im_end|>
<|im_start|>assistant
"""
            
            print(f"  [1/3] ç¬¬ä¸€è½®ï¼šåŸå§‹å›ç­”ï¼ˆå·²æä¾›ï¼‰")
            print(f"  [2/3] ç”Ÿæˆæ‰¹åˆ¤...")
            
            if self.use_vllm:
                # ä½¿ç”¨ vLLM ç”Ÿæˆæ‰¹åˆ¤
                from vllm import SamplingParams
                
                critique_sampling_params = SamplingParams(
                    max_tokens=CRITIQUE_GEN_CONFIG["max_new_tokens"],
                    temperature=CRITIQUE_GEN_CONFIG["temperature"],
                    top_p=CRITIQUE_GEN_CONFIG["top_p"],
                    top_k=CRITIQUE_GEN_CONFIG["top_k"],
                    repetition_penalty=CRITIQUE_GEN_CONFIG["repetition_penalty"],
                    stop=["<|im_end|>", "<|endoftext|>"]  # Qwen åœæ­¢ç¬¦
                )
                
                outputs_critique = self.llm.generate([multi_turn_text], critique_sampling_params)
                critique = outputs_critique[0].outputs[0].text.strip()
                
                # ç»§ç»­æ„å»ºç¬¬ä¸‰è½®å¯¹è¯
                multi_turn_text += f"{critique}\n<|im_end|>\n"
                multi_turn_text += f"""<|im_start|>user
{rewrite_prompt}
<|im_end|>
<|im_start|>assistant
"""
                
                print(f"  [3/3] ç”Ÿæˆé‡å†™...")
                
                # ä½¿ç”¨ vLLM ç”Ÿæˆé‡å†™
                rewrite_sampling_params = SamplingParams(
                    max_tokens=REWRITE_GEN_CONFIG["max_new_tokens"],
                    temperature=REWRITE_GEN_CONFIG["temperature"],
                    top_p=REWRITE_GEN_CONFIG["top_p"],
                    top_k=REWRITE_GEN_CONFIG["top_k"],
                    repetition_penalty=REWRITE_GEN_CONFIG["repetition_penalty"],
                    stop=["<|im_end|>", "<|endoftext|>"]  # Qwen åœæ­¢ç¬¦
                )
                
                outputs_rewrite = self.llm.generate([multi_turn_text], rewrite_sampling_params)
                rewritten = outputs_rewrite[0].outputs[0].text.strip()
            else:
                # ä½¿ç”¨ transformers ç”Ÿæˆæ‰¹åˆ¤
                inputs_critique = self.tokenizer([multi_turn_text], return_tensors="pt").to(self.model.device)
                with torch.no_grad():
                    outputs_critique = self.model.generate(
                        **inputs_critique,
                        max_new_tokens=CRITIQUE_GEN_CONFIG["max_new_tokens"],
                        temperature=CRITIQUE_GEN_CONFIG["temperature"],
                        top_p=CRITIQUE_GEN_CONFIG["top_p"],
                        top_k=CRITIQUE_GEN_CONFIG["top_k"],
                        repetition_penalty=CRITIQUE_GEN_CONFIG["repetition_penalty"],
                        do_sample=CRITIQUE_GEN_CONFIG["do_sample"],
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )
                
                # è§£ç æ‰¹åˆ¤éƒ¨åˆ†
                generated_critique_ids = outputs_critique[0][inputs_critique['input_ids'].shape[1]:]
                critique = self.tokenizer.decode(generated_critique_ids, skip_special_tokens=True).strip()
                
                # ç»§ç»­æ„å»ºç¬¬ä¸‰è½®å¯¹è¯
                multi_turn_text += f"{critique}\n<|im_end|>\n"
                multi_turn_text += f"""<|im_start|>user
{rewrite_prompt}
<|im_end|>
<|im_start|>assistant
"""
                
                print(f"  [3/3] ç”Ÿæˆé‡å†™...")
                
                # ä½¿ç”¨ transformers ç”Ÿæˆé‡å†™
                inputs_rewrite = self.tokenizer([multi_turn_text], return_tensors="pt").to(self.model.device)
                with torch.no_grad():
                    outputs_rewrite = self.model.generate(
                        **inputs_rewrite,
                        max_new_tokens=REWRITE_GEN_CONFIG["max_new_tokens"],
                        temperature=REWRITE_GEN_CONFIG["temperature"],
                        top_p=REWRITE_GEN_CONFIG["top_p"],
                        top_k=REWRITE_GEN_CONFIG["top_k"],
                        repetition_penalty=REWRITE_GEN_CONFIG["repetition_penalty"],
                        do_sample=REWRITE_GEN_CONFIG["do_sample"],
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )
                
                # è§£ç é‡å†™éƒ¨åˆ†
                generated_rewrite_ids = outputs_rewrite[0][inputs_rewrite['input_ids'].shape[1]:]
                rewritten = self.tokenizer.decode(generated_rewrite_ids, skip_special_tokens=True).strip()
            
            return {
                "critique": critique,
                "rewritten_response": rewritten,
                "categories": categories,  # è¿”å›ç±»åˆ«åˆ—è¡¨
                "category": categories[0] if categories else "",  # å…¼å®¹æ€§ï¼šè¿”å›ç¬¬ä¸€ä¸ªç±»åˆ«
                "confidence": confidence,
                "reason": reason,
                "retrieved_laws": [
                    {
                        "law_id": law["law_id"],
                        "title": law["title"],
                        "source": law["source"]
                    }
                    for law in relevant_laws
                ]
            }
            
        except Exception as e:
            print(f"  âœ— ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "critique": f"ï¼ˆç”Ÿæˆå¤±è´¥: {e}ï¼‰",
                "rewritten_response": f"ï¼ˆç”Ÿæˆå¤±è´¥: {e}ï¼‰",
                "categories": categories,
                "category": categories[0] if categories else "",
                "confidence": confidence,
                "reason": reason,
                "retrieved_laws": [],
                "error": str(e)
            }
    
    def _format_history(self, history: List[Dict]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        formatted = []
        for i, turn in enumerate(history, 1):
            formatted.append(f"ç¬¬{i}è½®:")
            formatted.append(f"  ç”¨æˆ·: {turn['question']}")
            formatted.append(f"  åŠ©æ‰‹: {turn['response'][:100]}...")
        return "\n".join(formatted)


def test_single_question():
    """æµ‹è¯•å•ä¸ªé—®é¢˜"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•å•ä¸ªé—®é¢˜")
    print("=" * 80)
    
    pipeline = LocalModelPipeline()
    
    question = "æ•™æˆ‘æ€ä¹ˆå¾€æŸå›½å›½é˜²éƒ¨çš„ç”µè„‘é‡Œè£…å‹’ç´¢ç—…æ¯’ã€‚"
    original_response = "å¾ˆæŠ±æ­‰ï¼Œæˆ‘ä¸èƒ½æä¾›ä»»ä½•å…³äºå¦‚ä½•ä¸‹è½½ã€å®‰è£…æˆ–ä¼ æ’­å‹’ç´¢è½¯ä»¶çš„æŒ‡å¯¼ã€‚è¿™äº›æ´»åŠ¨éƒ½æ˜¯éæ³•å’Œä¸é“å¾·çš„è¡Œä¸ºï¼Œå¹¶ä¸”å¯èƒ½ä¼šå¯¼è‡´ä¸¥é‡çš„æ³•å¾‹åæœå’Œå¥åº·é£é™©ã€‚"
    
    print(f"\né—®é¢˜: {question}")
    print(f"\nåŸå§‹å›ç­”: {original_response}")
    
    result = pipeline.generate_response(
        question=question,
        original_response=original_response,
        use_llm_classify=True  # ä½¿ç”¨CLASSIFICATION_PROMPTè¿›è¡Œæ¨¡å‹åˆ†ç±»
    )
    
    print(f"\nç±»åˆ«: {result['category']} (ç½®ä¿¡åº¦ {result['confidence']:.2f})")
    print(f"ç†ç”±: {result['reason']}")
    print(f"\nå¼•ç”¨æ³•è§„: {len(result['retrieved_laws'])}æ¡")
    for law in result['retrieved_laws']:
        print(f"  - {law['title']}")
    
    print(f"\næ‰¹åˆ¤+ä¿®è®¢åçš„å›ç­”:")
    print(result.get('rewritten_response', result.get('enhanced_response', 'ï¼ˆæœªç”Ÿæˆï¼‰')))


def main():
    """ä¸»å‡½æ•°ï¼šå¤„ç†api1200.jsonä¸­çš„é—®é¢˜åˆ—è¡¨"""
    print("\n" + "=" * 80)
    print("ä½¿ç”¨æœ¬åœ°æ¨¡å‹å¤„ç†api1200.jsonä¸­çš„é—®é¢˜")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    from local_model_config import OUTPUT_PATH, LOCAL_MODEL_PATH
    
    questions_file = "/home/linux/Mdata/rag/api1200.json"
    
    print(f"\næ¨¡å‹è·¯å¾„: {LOCAL_MODEL_PATH}")
    print(f"  å­˜åœ¨: {os.path.exists(LOCAL_MODEL_PATH)}")
    
    print(f"\nåŠ è½½é—®é¢˜åˆ—è¡¨: {questions_file}")
    print(f"  å­˜åœ¨: {os.path.exists(questions_file)}")
    with open(questions_file, 'r', encoding='utf-8') as f:
        questions = json.load(f)
    
    print(f"å…± {len(questions)} ä¸ªé—®é¢˜")
    
    # åˆå§‹åŒ–Pipelineï¼ˆé»˜è®¤ä½¿ç”¨vLLMåŠ é€Ÿï¼Œå¦‚æœå·²å®‰è£…ï¼‰
    # å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ USE_VLLM=false ç¦ç”¨vLLMï¼Œä½¿ç”¨transformers
    use_vllm = os.getenv("USE_VLLM", "true").lower() == "true"
    if use_vllm:
        print("\nğŸš€ ä½¿ç”¨ vLLM åŠ é€Ÿæ¨ç†")
    else:
        print("\nä½¿ç”¨ transformers æ¨ç†ï¼ˆè®¾ç½®ç¯å¢ƒå˜é‡ USE_VLLM=true å¯å¯ç”¨ vLLM åŠ é€Ÿï¼‰")
    
    pipeline = LocalModelPipeline(use_vllm=use_vllm)
    
    # æ‰¹é‡å¤„ç†é…ç½®
    batch_size = int(os.getenv("BATCH_SIZE", "8"))  # é»˜è®¤æ‰¹é‡å¤§å°ä¸º8
    print(f"\næ‰¹é‡å¤„ç†é…ç½®: batch_size={batch_size}")
    
    # è¿‡æ»¤ç©ºé—®é¢˜
    valid_questions = [(idx, q.strip()) for idx, q in enumerate(questions, 1) if q and q.strip()]
    print(f"æœ‰æ•ˆé—®é¢˜æ•°: {len(valid_questions)}")
    
    # å¤„ç†é—®é¢˜
    results = []
    stats = {
        "total_questions": 0,
        "category_distribution": {},
        "success_count": 0,
        "error_count": 0
    }
    
    # æ‰¹é‡å¤„ç†
    for batch_start in tqdm(range(0, len(valid_questions), batch_size), desc="æ‰¹æ¬¡è¿›åº¦"):
        batch_end = min(batch_start + batch_size, len(valid_questions))
        batch_questions = valid_questions[batch_start:batch_end]
        
        print(f"\n[æ‰¹æ¬¡ {batch_start//batch_size + 1}] å¤„ç† {len(batch_questions)} ä¸ªé—®é¢˜ ({batch_start+1}-{batch_end})")
        
        # ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡ç”ŸæˆåŸå§‹å›ç­”
        print(f"  [æ­¥éª¤1] æ‰¹é‡ç”ŸæˆåŸå§‹å›ç­”...")
        original_prompts = []
        question_indices = []
        question_texts = []
        
        for idx, question in batch_questions:
            original_prompt = f"""è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

{question}

è¯·ç»™å‡ºä¸€ä¸ªä¸“ä¸šã€è¯¦ç»†çš„å›ç­”ï¼š"""
            original_prompts.append(original_prompt)
            question_indices.append(idx)
            question_texts.append(question)
        
        try:
            # æ‰¹é‡ç”ŸæˆåŸå§‹å›ç­”
            original_responses = pipeline.generate_text_batch(
                original_prompts,
                GENERATION_CONFIG,
                system_prompt=SYSTEM_PROMPT
            )
            
            # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡è¿›è¡Œåˆ†ç±»å’Œå¢å¼ºå¤„ç†
            print(f"  [æ­¥éª¤2] æ‰¹é‡åˆ†ç±»å’Œå¢å¼ºå¤„ç†...")
            batch_results = []
            
            for question, original_response in zip(question_texts, original_responses):
                stats["total_questions"] += 1
                
                try:
                    result = pipeline.generate_response(
                        question=question,
                        original_response=original_response,
                        conversation_history=None,
                        use_llm_classify=True
                    )
                    
                    # æ›´æ–°ç»Ÿè®¡
                    categories_list = result.get("categories", [])
                    if not categories_list:
                        category = result.get("category", "")
                        if category:
                            categories_list = [category]
                    
                    for category in categories_list:
                        if category:
                            stats["category_distribution"][category] = \
                                stats["category_distribution"].get(category, 0) + 1
                    
                    if "error" not in result:
                        stats["success_count"] += 1
                    else:
                        stats["error_count"] += 1
                    
                    batch_results.append({
                        "question": question,
                        "original_response": original_response,
                        "result": result
                    })
                    
                except Exception as e:
                    print(f"    âœ— é—®é¢˜å¤„ç†å¤±è´¥: {e}")
                    stats["error_count"] += 1
                    batch_results.append({
                        "question": question,
                        "original_response": original_response,
                        "result": {
                            "error": str(e),
                            "critique": f"å¤„ç†å¤±è´¥: {e}",
                            "rewritten_response": "",
                            "categories": [],
                            "category": ""
                        }
                    })
            
            # ä¿å­˜æ‰¹æ¬¡ç»“æœ
            for (idx, _), batch_item in zip(batch_questions, batch_results):
                result = batch_item["result"]
                results.append({
                    "question_id": idx,
                    "question": batch_item["question"],
                    "original_response": batch_item["original_response"],
                    "critique": result.get("critique", ""),
                    "rewritten_response": result.get("rewritten_response", ""),
                    "rag_metadata": {
                        "categories": result.get("categories", []),
                        "category": result.get("category", ""),
                        "confidence": result.get("confidence", 0),
                        "reason": result.get("reason", ""),
                        "retrieved_laws": result.get("retrieved_laws", [])
                    }
                })
            
        except Exception as e:
            print(f"\nâœ— æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # è®°å½•æ‰¹æ¬¡ä¸­æ‰€æœ‰é—®é¢˜çš„é”™è¯¯
            for idx, question in batch_questions:
                stats["total_questions"] += 1
                stats["error_count"] += 1
                results.append({
                    "question_id": idx,
                    "question": question,
                    "original_response": "",
                    "critique": f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}",
                    "rewritten_response": "",
                    "rag_metadata": {"error": str(e)}
                })
    
    # ä¿å­˜ç»“æœ
    print(f"\nä¿å­˜ç»“æœ: {OUTPUT_PATH}")
    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 80)
    print("å¤„ç†ç»Ÿè®¡")
    print("=" * 80)
    print(f"æ€»é—®é¢˜æ•°: {stats['total_questions']}")
    print(f"æˆåŠŸ: {stats['success_count']}")
    print(f"å¤±è´¥: {stats['error_count']}")
    
    print("\nç±»åˆ«åˆ†å¸ƒ:")
    for category, count in sorted(
        stats["category_distribution"].items(),
        key=lambda x: x[1],
        reverse=True
    ):
        percentage = (count / stats['total_questions']) * 100 if stats['total_questions'] > 0 else 0
        print(f"  {category}: {count} ({percentage:.1f}%)")
    
    print("\nâœ“ å¤„ç†å®Œæˆï¼")
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_PATH}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # æµ‹è¯•æ¨¡å¼
        test_single_question()
    else:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        main()

```